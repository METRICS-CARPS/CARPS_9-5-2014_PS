---
title: "CARPS Reproducibility Report"
output:
  html_document:
    toc: true
    toc_float: true
---

# Report Details

```{r}
articleID <- "9-5-2014" # insert the article ID code here e.g., "10-3-2015_PS"
reportType <- 'final'
pilotNames <- "Kari Leibowitz, Yochai Shavit" # insert the pilot's name here e.g., "Tom Hardwicke". If there are multiple pilots enter both names in a character string e.g., "Tom Hardwicke, Bob Dylan"
copilotNames <- "Kyle MacDonald" # insert the co-pilot's name here e.g., "Michael Frank". If there are multiple co-pilots enter both names in a character string e.g., "Tom Hardwicke, Bob Dylan"
pilotTTC <- 300 # insert the pilot's estimated time to complete (in minutes, fine to approximate) e.g., 120
copilotTTC <- 240 # insert the co- pilot's estimated time to complete (in minutes, fine to approximate) e.g., 120
pilotStartDate <- as.Date("11/1/17", format = "%m/%d/%y") # insert the pilot's start date in US format e.g., as.Date("01/25/18", format = "%m/%d/%y")
copilotStartDate <- as.Date("6/15/18", format = "%m/%d/%y") # insert the co-pilot's start date in US format e.g., as.Date("01/25/18", format = "%m/%d/%y")
completionDate <- as.Date("", format = "%m/%d/%y") # copilot insert the date of final report completion (after any necessary rounds of author assistance) in US format e.g., as.Date("01/25/18", format = "%m/%d/%y")
```

-------

#### Methods summary: 

In the present study, Cogsdill et al. presented participants in four age groups (3-4 year olds (n=37), 5-6 year olds(n=50), 7-10 year olds (n=54)- tested in lab or at a local museum and adults (n=99, tested online)) with pairs of faces that were pre-determined to be high (+3 s.d) or low (-3 s.d) in three traits: trustworthiness (measured as nice vs. mean), competence (measured as smart vs. not smart), or dominance (measured as strong vs. not strong). For each trait, 3 "high on trait" and 3 "low on trait" faces were selected and each pair consisted of one face was a "high on trait" face and the other a "low on trait" face. All possible 9 iterations were presented to participants in a counter balanced order across traits. The authors assessed the ability of each age group's members to accurately identify which of two faces was high in a particular trait, by asking participants to indicate which of these two people is very nice/strong/smart. The authors were interested in the consistency of judgements across age groups in terms of the ability to "correctly" identify traits of faces based on a pre-determined consensus. 

------

#### Target outcomes: 

For this article we focused on the findings reported in the results section of Experiment 1.

Specifically, we attempted to reproduce all descriptive and inferential analyses reported in the text below and associated tables/figures (broken down by procedure):

> Figures 2, 3, and 4 summarize results for all age groups and traits; higher percentages of expected responses (i.e., those predicted on the basis of prior data--e.g., that trustworthy faces would be identified as nice and untrustworthy faces as mean) indicate stronger consensus. Combined, all four age groups showed significant consensus compared with chance (50%) when identifying faces as mean or nice (93%; Fig. 2), strong or not strong (85%; Fig. 3), and smart or not smart (76%; Fig. 4). Critically, all age groups attributed all three traits with significant consensus, ps < .001, ds > 1.08. However, an analysis of variance (ANOVA) revealed a significant main effect of age group, F(3, 236) = 17.91, p < .001. Although 3- to 4-year-olds responded with robust and adult like consensus (72% across all traits), they were less consistent than 5- to 6-year-olds (81%), 7- to 10-year-olds (88%), and adults (89%). One-way ANOVAs followed by post hoc tests with Sidak corrections for multiple comparisons were used to analyze age differences for each trait. These analyses revealed that when attributing both trustworthiness and dominance, 3- to 4-year-olds were less consistent than all other age groups (all ps < .01, ds > 0.59), which exhibited equivalent consistency (all ps > .23, ds < 0.40).

------

```{r global_options, include=FALSE}
knitr::opts_chunk$set(echo=TRUE, warning=FALSE, message=FALSE)
```

# Step 1: Load packages

```{r}
library(tidyverse) # for data munging
library(knitr) # for kable table formating
library(haven) # import and export 'SPSS', 'Stata' and 'SAS' Files
library(readxl) # import excel files
library(CARPSreports) # custom report functions
library(effsize) #used to calculate effect size
library(dunn.test)#used to do post hoc comparisons with Sidak's correction
library(car)
library(lsr) # using this to get effect sizes
library(broom)
```

```{r}
# Prepare report object. This will be updated automatically by the reproCheck function each time values are compared.
reportObject <- data.frame(dummyRow = TRUE, reportedValue = NA, obtainedValue = NA, valueType = NA, 
                           percentageError = NA, comparisonOutcome = NA, eyeballCheck = NA)
```

# Step 2: Load data

```{r}
d <- read_xlsx("DATA/Cogsdill_FaceTrait_Experiment1Public_04Feb14.xlsx")
```

# Step 3: Tidy data

Clean up column names and gather the ratings for each age group.

```{r}
colnames(d) <- colnames(d) %>% 
  str_replace(pattern = " ", "_") %>%
  str_to_lower()

d_tidy <- d %>% gather(key = attribute, value = rating, nice:smart)
```

# Step 4: Run analysis

### Descriptive statistics

Compute summary statisics for each age group and attribute.

```{r}
ms <- d_tidy %>% 
  group_by(age_group, attribute) %>% 
  summarise(m = mean(rating),
            stdev = sd(rating),
            n = n(),
            sem = stdev / sqrt(n))

ms %>% kable(digits = 2)
```

#### Reproduce Figure 2

![](img/faces_figure2_trBYgroup.png)

```{r figure 2}
ms %>% 
  filter(attribute == "nice") %>% 
  ggplot(aes(x = age_group, y = m)) +
  geom_bar(stat = "identity", fill="grey", color="black", width = 0.5) +
  geom_errorbar(aes(ymin = m - sem, ymax= m + sem), width=0.2)+
  ylim(0,1)+
  theme_bw()+
  labs(x="Age Group", y="Percent expected responses (Trustworthy='Nice')")+
  ggtitle("figure 2 reproduced: 'nice' by Age group")
```

Add eyeball repro checks for the mean values in Figure 2.

```{r}
reportObject <- reproCheck(reportedValue = "0.77", obtainedValue = 0.77, valueType = 'mean', eyeballCheck = TRUE)
reportObject <- reproCheck(reportedValue = "0.9", obtainedValue = 0.9, valueType = 'mean', eyeballCheck = TRUE)
reportObject <- reproCheck(reportedValue = "0.95", obtainedValue = 0.95, valueType = 'mean', eyeballCheck = TRUE)
reportObject <- reproCheck(reportedValue = "0.93", obtainedValue = 0.93, valueType = 'mean', eyeballCheck = TRUE)
```

Rerpoducing this finding:

> "Combined, all four age groups showed significant consensus compared with chance (50%) when identifying faces as mean or nice (93%; Fig. 2)... Cogsdill et al., page 1134"

```{r niceness average}
mean_nice <- d_tidy %>% filter(attribute == "nice") %>% pull(rating) %>% mean()
reportObject <- reproCheck(reportedValue = "0.93", obtainedValue = mean_nice, valueType = 'mean')
```

#### Reproduce Figure 3 

![](img/faces_figure3_domBYgroup.png)

```{r figure 3}
ms %>% 
  filter(attribute == "strong") %>% 
  ggplot(aes(x = age_group, y = m)) +
  geom_bar(stat = "identity", fill="grey", color="black", width = 0.5) +
  geom_errorbar(aes(ymin = m - sem, ymax = m + sem), width=0.2)+
  ylim(0,1)+
  theme_bw()+
  labs(x="Age Group", y="Percent expected responses (Dominant='Nice')")+
  ggtitle("figure 2 reproduced: 'strong' by Age group")
```

Add eyeball repro checks for the mean values in Figure 3.

```{r}
reportObject <- reproCheck(reportedValue = "0.63", obtainedValue = 0.63, valueType = 'mean', eyeballCheck = TRUE)
reportObject <- reproCheck(reportedValue = "0.85", obtainedValue = 0.85, valueType = 'mean', eyeballCheck = TRUE)
reportObject <- reproCheck(reportedValue = "0.85", obtainedValue = 0.85, valueType = 'mean', eyeballCheck = TRUE)
reportObject <- reproCheck(reportedValue = "0.9", obtainedValue = 0.9, valueType = 'mean', eyeballCheck = TRUE)
```

Next, we try to reproduce this finding:

> "Combined, all four age groups showed significant consensus compared with chance (50%) when identifying faces as...strong or not strong (85%; Fig. 3)... Cogsdill et al., page 1134"

```{r niceness average 2}
mean_strong <- d_tidy %>% filter(attribute == "strong") %>% pull(rating) %>% mean()
reportObject <- reproCheck(reportedValue = "0.85", obtainedValue = mean_strong, valueType = 'mean')
```

#### Reproduce Figure 4

![](img/faces_figure4_compBYgroup.png)

```{r figure 4}
ms %>% 
  filter(attribute == "smart") %>% 
  ggplot(aes(x = age_group, y = m)) +
  geom_bar(stat = "identity", fill="grey", color="black", width = 0.5) +
  geom_errorbar(aes(ymin = m - sem, ymax = m + sem), width=0.2)+
  ylim(0,1)+
  theme_bw()+
  labs(x="Age Group", y="Percent expected responses (Competent='Smart')")+
  ggtitle("figure 2 reproduced: 'smart' by Age group")
```

Add eyeball repro checks for the mean values in Figure 3.

```{r}
reportObject <- reproCheck(reportedValue = "0.65", obtainedValue = 0.65, valueType = 'mean', eyeballCheck = TRUE)
reportObject <- reproCheck(reportedValue = "0.65", obtainedValue = 0.65, valueType = 'mean', eyeballCheck = TRUE)
reportObject <- reproCheck(reportedValue = "0.8", obtainedValue = 0.8, valueType = 'mean', eyeballCheck = TRUE)
reportObject <- reproCheck(reportedValue = "0.77", obtainedValue = 0.77, valueType = 'mean', eyeballCheck = TRUE)
```

Next, we try to reproduce this finding:

> "Combined, all four age groups showed significant consensus compared with chance (50%) when identifying faces as...smart or not smart (76%; Fig. 4). Cogsdill et al., page 1134"

```{r smartness average}
mean_smart <- d_tidy %>% filter(attribute == "smart") %>% pull(rating) %>% mean()
reportObject <- reproCheck(reportedValue = "0.76", obtainedValue = mean_smart, valueType = 'mean')
```

#### Check consistency across age groups

> "Although 3- to 4-year-olds responded with robust and adult like consensus (72% across all traits), they were less consistent than 5- to 6-year-olds (81%), 7- to 10-year-olds (88%), and adults (89%). Cogsdill et al., page 1134"

```{r consistency by age group across traits}
by_age_ms <- d_tidy %>% 
  group_by(age_group) %>% 
  summarise(m = mean(rating)) 

by_age_ms %>% kable(digits = 2)
```

All consistent with statistics reported in paper. Add repro checks for these values. 

```{r}
reportObject <- reproCheck(reportedValue = "0.72", obtainedValue = by_age_ms$m[1], valueType = 'mean')
reportObject <- reproCheck(reportedValue = "0.81", obtainedValue =  by_age_ms$m[2], valueType = 'mean')
reportObject <- reproCheck(reportedValue = "0.88", obtainedValue =  by_age_ms$m[3], valueType = 'mean')
reportObject <- reproCheck(reportedValue = "0.89", obtainedValue =  by_age_ms$m[4], valueType = 'mean')
```

### Inferential statistics

T-tests against chance, reproducing this claim:

> "Combined, all four age groups showed significant consensus compared with chance (50%) when identifying faces as mean or nice , strong or not strong, and smart or not smart."

However, the authors did not report which test they used to come up with the conclusion that this percentage is significantly different from 50%. 

**INSUFFICIENT INFORMATION error**

```{r one-sample t test aginst 0.5 all conditions and age groups}
# wrapper function for t-test
t_test_fun <- function(df) {
  m <- t.test(df$rating, mu = 0.5, alternative = "two.sided")
  m %>% broom::glance()
}
# nest the data for each attribute
d_by_attr <- d_tidy %>% 
  group_by(attribute) %>% 
  nest()

# map the t-test function against random responding (0.5) to each attribute
d_by_attr <- d_by_attr %>% 
  mutate(t_test = purrr::map(data, t_test_fun)) 

by_attr_t <- d_by_attr %>% unnest(t_test) %>% select(-data)
by_attr_t %>% kable(digits = 2)
```

Exract and check the t-valuea for each attribute.

```{r}
t_mean <- by_attr_t %>% filter(attribute == "strong") %>% pull(statistic) %>% round(2)
t_smart <- by_attr_t %>% filter(attribute == "smart") %>% pull(statistic) %>% round(2)
t_nice <- by_attr_t %>% filter(attribute == "nice") %>% pull(statistic) %>% round(2)
```

All groups were significantly different from random responding. Can't check these values because nothing was reported in the paper.

Next, we try to reproduce this test:

> "Critically, all age groups attributed all three traits with significant consensus, ps < .001, ds > 1.08."

**INSUFFICIENT INFORMATION ERROR**

We attempted to replicate this finding using our best guess of what test to use, but after calculating this realized that this probably entailed "lengthy guesswork" on our part. Thus, we have marked this as an insufficient information error but also included the tests we attempted to run below. 

Note: the authors don't specify what analytical test was used to determine consensus across age groups and there is some ambiguity in the phrasing here. We took this to mean that within each age group, ratings for each of the three attributes (nice, smart, strong) were consistent. Some thoughts on how to implement this:

  * In order to examine this, we looked at t tests and effect sizes for the percentage of correct and incorrect identifications using the "overall" variable across all attributes within each age group. However, it is unclear from the manuscript whether this was the correct approach.
  * We also took an alternative approach, interpreting "consensus" as percent overall greater than 50%. 

Run test:

```{r}
## test

## repro check values

```


#### Reproduce ANOVAs

We next attempt to reproduce this claim:

> "However, an analysis of variance (ANOVA) revealed a significant main effect of age group, F(3, 236) = 17.91, p < .001. Although 3- to 4-year-olds responded with robust and adult like consensus (72% across all traits), they were less consistent than 5- to 6-year-olds (81%), 7- to 10-year-olds (88%), and adults (89%). "

```{r}
anova1 <- d_tidy %>% 
  distinct(participant_id, overall, age_group) %>% 
  aov(overall ~ age_group, data = .) %>% 
  broom::glance()

anova1 %>% kable(digits = 2, caption="ANOVA to compare group means on overall percent correct")
```

Add repro check values:

```{r}
reportObject <- reproCheck(reportedValue = "17.91", obtainedValue = anova1$statistic, valueType = 'F')
```

Next, we attempt to reproduce this claim:

> "One-way ANOVAs followed by post hoc tests with Sidak corrections for multiple comparisons were used to analyze age differences for each trait. These analyses revealed that when attributing both trustworthiness and dominance, 3- to 4-year-olds were less consistent than all other age groups (all ps < .01, ds > 0.59), which exhibited equivalent consistency (all ps > .23, ds < 0.40)."

Note: trustworthiness (measured as nice vs. mean), competence (measured as smart vs. not smart), or dominance (measured as strong vs. not strong)

```{r final anovas}
fit_anova <- function(df) {
  m <- aov(rating ~ age_group, data = df)
  m %>% broom::glance()
}

d_by_attr <- d_by_attr %>% 
  mutate(anova_fit = purrr::map(data, fit_anova))

d_anovas <- d_by_attr %>% unnest(anova_fit) %>% select(-data, -t_test)
d_anovas %>% kable(digits = 2)
```

Next, we perform the follow-up tests with Sidak correction for multiple comparisons.

```{r dunn tests}
run_dunn <- function(df) {
  result <- dunn.test(x = df$rating, g = df$age_group, method = "sidak")
  tibble(
    comparison = result$comparisons,
    z = result$Z,
    p = result$P,
    p_adj = result$P.adjusted
  )
}

d_by_attr <- d_by_attr %>% 
  mutate(dunn_fit = purrr::map(data, run_dunn))

ms_dunn <- d_by_attr %>% unnest(dunn_fit)
ms_dunn %>% kable()
```

Check p values for just the 3-4 year-olds against the other groups for nice and strong attributes.

```{r}
three_four_contrast <- ms_dunn %>% 
  filter(str_detect(comparison, "3-4 year olds"),
         attribute %in% c("nice", "strong")) 

three_four_contrast %>% kable(digits = 3)
```

Extract the largest adjusted p-value to compare against the reported threshold of $p <. 01$.

```{r}
three_four_contr_p <- three_four_contrast %>% pull(p_adj) %>% max()
reportObject <- reproCheck(reportedValue = "< 0.01", obtainedValue = three_four_contr_p, valueType = 'p', 
                           eyeballCheck = FALSE)
```

Next, we try to reproduce this claim:

> all other age groups ... which exhibited equivalent consistency (all ps > .23, ds < 0.40)."

We filter the dunn tests to remove comparisons using the 3-4 year-olds and then compare the smallest p-value to the reported threshold.

```{r}
other_groups_contrast <- ms_dunn %>% 
  filter(!(str_detect(comparison, "3-4 year olds")),
         attribute %in% c("nice", "strong")) 

other_groups_contrast %>% kable(digits = 3)
```

Extract the p-values that are below the stated threshold in the paper ($p > 0.23$).

```{r}
nice_five_seven <- other_groups_contrast %>%  pull(p_adj) %>% .[1]
strong_five_adults <- other_groups_contrast %>%  pull(p_adj) %>% .[5]
strong_seven_adults <- other_groups_contrast %>%  pull(p_adj) %>% .[6]

# add repro checks
reportObject <- reproCheck(reportedValue = "p > 0.23", obtainedValue = nice_five_seven, 
                           valueType = 'p', eyeballCheck = FALSE)
reportObject <- reproCheck(reportedValue = "p > 0.23", obtainedValue = strong_five_adults, 
                           valueType = 'p', eyeballCheck = FALSE)
reportObject <- reproCheck(reportedValue = "p > 0.23", obtainedValue = strong_seven_adults, 
                           valueType = 'p', eyeballCheck = FALSE)
```

#### Reproduce effect sizes

> "These analyses revealed that when attributing both trustworthiness and dominance, 3- to 4-year-olds were less consistent than all other age groups (all ps < .01, ds > 0.59), which exhibited equivalent consistency (all ps > .23, ds < 0.40)."

**INSUFFICIENT INFORMATION ERROR**

Here, we run into the same issue we ran into before: without more information about which specific tests were used. However, since the authors report effect sizes using Cohen's d, which is the effect size for t-test, and since these effect sizes do not depend on correction of p.values, we proceeded (with caution) to obtain Cohen's d comparing 3-4 year olds means for percent correct on 'trustworthiness' and 'dominance' with those of all other age groups.

```{r effect sizes}
est_coh_d <- function(df) {
  cohensD(rating ~ age_group, data = df)  
}
nest_compute_coh_d <- function(df) {
  d_by_attr <- df %>% 
    group_by(attribute) %>% 
    nest()
  
  d_by_attr %>% 
    mutate(
      eff_size = purrr::map(data, est_coh_d)
    ) %>% 
    unnest(eff_size) %>% 
    select(-data)
}

df_eff_size <- d_tidy %>%
  filter(age_group %in% c("3-4 year olds", "5-6 year olds")) %>% 
  nest_compute_coh_d() %>% 
  mutate(comparison = "3-4 vs. 5-6")

df_eff_size <- d_tidy %>%
  filter(age_group %in% c("3-4 year olds", "7-10 year olds")) %>% 
  nest_compute_coh_d() %>% 
  mutate(comparison = "3-4 vs. 7-10") %>% 
  bind_rows(df_eff_size)

df_eff_size <- d_tidy %>%
  filter(age_group %in% c("3-4 year olds", "Adults")) %>% 
  nest_compute_coh_d() %>% 
  mutate(comparison = "3-4 vs. Adults") %>% 
  bind_rows(df_eff_size)

df_eff_size %>% kable(caption = "Effect sizes for 3-4 year-olds compared to other age groups")
```

Get smallest effect size (just for trustworthiness and dominance) and compare to stated threshold $ds > 0.59$. 

```{r}
min_eff_size <- df_eff_size %>%
  filter(attribute %in% c("strong", "nice")) %>% 
  pull(eff_size) %>% 
  min()

reportObject <- reproCheck(reportedValue = "> 0.59", obtainedValue = min_eff_size, valueType = 'd',
                           eyeballCheck = TRUE)
```

Next, we try to reproduce the effect sizes for all other group comparisons and compare against the reported threshold $ds < 0.40$.

```{r}
df_eff_size_other <- d_tidy %>%
  filter(age_group %in% c("5-6 year olds", "7-10 year olds")) %>% 
  nest_compute_coh_d() %>% 
  mutate(comparison = "5-6 vs. 7-10")

df_eff_size_other <- d_tidy %>%
  filter(age_group %in% c("5-6 year olds", "Adults")) %>% 
  nest_compute_coh_d() %>% 
  mutate(comparison = "5-6 vs. Adults") %>% 
  bind_rows(df_eff_size_other)

df_eff_size_other <- d_tidy %>%
  filter(age_group %in% c("7-10 year olds", "Adults")) %>% 
  nest_compute_coh_d() %>% 
  mutate(comparison = "7-10 vs. Adults") %>% 
  bind_rows(df_eff_size_other)

df_eff_size_other %>% kable(caption = "Effect sizes for all other group comparisons")
```

Get smallest effect size (just for trustworthiness and dominance) and compare to stated threshold $ds < 0.40$. 

```{r}
max_eff_size <- df_eff_size_other %>%
  filter(attribute %in% c("strong", "nice")) %>% 
  pull(eff_size) %>% 
  max()

reportObject <- reproCheck(reportedValue = "0.40", obtainedValue = max_eff_size, valueType = 'd')
```

The largest effect size for all other group comparisions is slightly larger than that reported in the paper. But this is a minor numerical error.

# Step 5: Conclusion

We were able to reproduce the descriptive statistics and the three key figures. But there were four insufficient information errors that prevented a full reproducbility check: 

  * the statistical test used to compare against chance responding
  * the statistical test used to test for similar consensus across age groups
  * the exact comparisons that were included in the claim about groups showing "similar consistency"
  * the measure of effect size that was used

In addition, we were unable to reproduce several statistical results related to the following claim (see the reportObject for more inf:

> These analyses revealed that when attributing both trustworthiness and dominance, 3- to 4-year-olds were less consistent than all other age groups (all ps < .01, ds > 0.59), which exhibited equivalent consistency (all ps > .23, ds < 0.40).

Specifically, we found:

  * a 3-4 year-old age group comparison where $p > .01$, which represents a decision error (false positive)
  * three "other" age group comparisons where $p < .05$, which represent decision errors (false negatives)

Finally, we found minor numerical differences in our estimates of effect sizes, which are probably caused by computational differences in software or typos. 

```{r}
Author_Assistance = FALSE # was author assistance provided? (if so, enter TRUE)

Insufficient_Information_Errors <- 3 # how many discrete insufficient information issues did you encounter?

# Assess the causal locus (discrete reproducibility issues) of any reproducibility errors. Note that there doesn't necessarily have to be a one-to-one correspondance between discrete reproducibility issues and reproducibility errors. For example, it could be that the original article neglects to mention that a Greenhouse-Geisser correct was applied to ANOVA outcomes. This might result in multiple reproducibility errors, but there is a single causal locus (discrete reproducibility issue).

locus_typo <- NA # how many discrete issues did you encounter that related to typographical errors?
locus_specification <- NA # how many discrete issues did you encounter that related to incomplete, incorrect, or unclear specification of the original analyses?
locus_analysis <- NA # how many discrete issues did you encounter that related to errors in the authors' original analyses?
locus_data <- NA # how many discrete issues did you encounter that related to errors in the data files shared by the authors?
locus_unidentified <- NA # how many discrete issues were there for which you could not identify the cause

# How many of the above issues were resolved through author assistance?
locus_typo_resolved <- NA # how many discrete issues did you encounter that related to typographical errors?
locus_specification_resolved <- NA # how many discrete issues did you encounter that related to incomplete, incorrect, or unclear specification of the original analyses?
locus_analysis_resolved <- NA # how many discrete issues did you encounter that related to errors in the authors' original analyses?
locus_data_resolved <- NA # how many discrete issues did you encounter that related to errors in the data files shared by the authors?
locus_unidentified_resolved <- NA # how many discrete issues were there for which you could not identify the cause

Affects_Conclusion <- NA # Do any reproducibility issues encounter appear to affect the conclusions made in the original article? This is a subjective judgement, but you should taking into account multiple factors, such as the presence/absence of decision errors, the number of target outcomes that could not be reproduced, the type of outcomes that could or could not be reproduced, the difference in magnitude of effect sizes, and the predictions of the specific hypothesis under scrutiny.
```

```{r}
reportObject <- reportObject %>%
  filter(dummyRow == FALSE) %>% # remove the dummy row
  select(-dummyRow) %>% # remove dummy row designation
  mutate(articleID = articleID) %>% # add the articleID 
  select(articleID, everything()) # make articleID first column

# decide on final outcome
if(any(reportObject$comparisonOutcome %in% c("MAJOR_ERROR", "DECISION_ERROR")) | Insufficient_Information_Errors > 0){
  finalOutcome <- "Failure without author assistance"
  if(Author_Assistance == T){
    finalOutcome <- "Failure despite author assistance"
  }
}else{
  finalOutcome <- "Success without author assistance"
  if(Author_Assistance == T){
    finalOutcome <- "Success with author assistance"
  }
}

# collate report extra details
reportExtras <- data.frame(articleID, pilotNames, copilotNames, pilotTTC, copilotTTC, pilotStartDate, copilotStartDate, completionDate, Author_Assistance, finalOutcome, Insufficient_Information_Errors, locus_typo, locus_specification, locus_analysis, locus_data, locus_unidentified, locus_typo_resolved, locus_specification_resolved, locus_analysis_resolved, locus_data_resolved, locus_unidentified_resolved)

# save report objects
if(reportType == "pilot"){
  write_csv(reportObject, "pilotReportDetailed.csv")
  write_csv(reportExtras, "pilotReportExtras.csv")
}

if(reportType == "final"){
  write_csv(reportObject, "finalReportDetailed.csv")
  write_csv(reportExtras, "finalReportExtras.csv")
}
```

# Session information

```{r session_info, include=TRUE, echo=TRUE, results='markup'}
devtools::session_info()
```

